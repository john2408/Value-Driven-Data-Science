{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention \n",
    "\n",
    "Ref: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
    "\n",
    "We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input’s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding an Input Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use this dictionary to assign an integer index to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a 16-dimensional embedding such that each input word is represented by a 16-dimensional vector. Since the sentence consists of 6 words, this will result in a **6×16**\n",
    "-dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = 16 # size of word vector embedding\n",
    "embed = torch.nn.Embedding(6, 16)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product self attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention utilizes three weight matrices, referred as $\\mathbf{W_{q}}$, $\\mathbf{W_{k}}$ and $\\mathbf{W_{V}}$, which are adjusted as model parameters during the model training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively. \n",
    "\n",
    "The respective query, key and value sequences are obtained via matrix multiplication between the weight matrices $\\mathbf{W}$ and the embedded inputs $\\mathbf{x}$:\n",
    "\n",
    "- Query sequence : $\\mathbf{q^{(i)} = W_{q}x^{i}} $ for $i \\in [1,T]$\n",
    "- Key sequence : $\\mathbf{k^{(i)} = W_{k}x^{i}} $ for $i \\in [1,T]$\n",
    "- Value sequence : $\\mathbf{v^{(i)} = W_{v}x^{i}} $ for $i \\in [1,T]$\n",
    "\n",
    "Where $i$ refers to the token index position in the input sequence, which has lenght $T$.\n",
    "\n",
    "Let ${d}$ be the size of each word vector $\\mathbf{x}$. Therefore, both $q^{(i)}$ and $k^{(i)}$ are vectors of dimension $d_{k}$. The projection matrices $\\mathbf{W_{q}}$ and $\\mathbf{W_{k}}$ have shape $d_{k} \\times d$, while $\\mathbf{W_{v}}$ has shape $d_{v} \\times d$.\n",
    "\n",
    "<img src=\"./img/attention-matrices.png\" alt=\"alt text\"  style=\"height: 50%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are computing the dot-product between the query and key vectors, these two vector have to contain the same number of elements ($d_{q} = d_{k}$). However, the number of elements in the value vector $v^{i}$, which determines the size of the resulting context vector, is arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Unnormalized Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s suppose we are interested in computing the attention-vector for the second input element – the second input element acts as the query here:\n",
    "\n",
    "<img src=\"./img/query.png\" alt=\"alt text\" \n",
    "  style=\"height: 50%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generalize this to compute $th$ remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights $\\omega$ :\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnormalized attention weights $\\omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/omega.png\" alt=\"alt text\" \n",
    "  style=\"height: 50%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 50%;\"/>\n",
    "\n",
    "For which, $w_{i,j}$ is the dot product between the query and the key sequences, $w_{i,j} = \\mathbf{q^{(i)^{T}}k^{j}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unnormalized attention weight for the query and the 5th input element will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1466, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the $omega$ values for all input tokens as illustrated before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Attention Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent step in self-attention is to normalize the unnormalized attention weights, $ω$\n",
    ", to obtain the normalized attention weights, $α$\n",
    ", by applying the softmax function. Additionally, $\\frac{1}{\\sqrt{d_{k}}}$\n",
    " is used to scale ω\n",
    " before normalizing it through the softmax function, as shown below:\n",
    "\n",
    "\n",
    "<img src=\"./img/attention-scores.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling by $d_{k}$ ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too small or too large, which could lead to numerical instability or affect the model’s ability to converge during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-weighted version of orignal query $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last step is to compute the context vector $\\mathbf{z^{(2)}}$, which is an attention-weighted version of our original query input $\\mathbf{x^{(2)}}$\n",
    ", including all the other input elements as its context via the attention weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/context-vector.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this output vector has more dimensions $(d_{v}=28\n",
    ")$ than the original input vector $(d=16\n",
    ")$ since we specified $d_{v}>d$\n",
    " earlier; however, the embedding size choice is arbitrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "In the scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention. The figure below summarizes this single attention head we covered previously:\n",
    "\n",
    "<img src=\"./img/single-head.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>\n",
    "\n",
    "As its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks.\n",
    "\n",
    "<img src=\"./img/multi-head.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this in code, suppose we have 3 attention heads, so we now extend the $d^{´} \\times d$ dimensional weight matrices so $3 \\times d^{'}\\times d$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, each query element is now $3 \\times d_{q}$ dimensional, where $d_{q}=24$\n",
    " (here, let’s keep the focus on the 3rd element corresponding to index position 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([3, 24, 16]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape, multihead_W_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 24, 16])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_W_key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain key and values in a similar way \n",
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these key and value elements are specific to the query element. But, similar to earlier, we will also need the value and keys for the other sequence elements in order to compute the attention scores for the query. We can do this is by expanding the input sequence embeddings to size 3, i.e., the number of attention heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 6])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
       "        [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
       "        [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the input is three dimensional\n",
    "stacked_inputs[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute all the keys and values using torch.bmm() ( batch matrix multiplication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24, 16]), torch.Size([3, 16, 6]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_W_key.shape, stacked_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tensors that represent the three attention heads in their first dimension. The third and second dimensions refer to the number of words and the embedding size, respectively. To make the values and keys more intuitive to interpret, we will swap the second and third dimensions, resulting in tensors with the same dimensional structure as the original input sequence, embedded_sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "# multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "# multihead_values = multihead_values.permute(0, 2, 1)\n",
    "# print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "# print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we follow the same steps as previously to compute the unscaled attention weights $ω$ and attention weights $α$, followed by the scaled-softmax computation to obtain an $h \\times d_{v}$ (here: $3 \\times d_{v}$) dimensional context vector $\\mathbf{z}$ for the input element $x^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24]), torch.Size([6, 24]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.shape, keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 24]), torch.Size([3, 24, 6]), torch.Size([6, 24, 3]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_query_2.shape, multihead_keys.shape, multihead_keys.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_omega_2 = multihead_query_2.matmul(multihead_keys)\n",
    "print(multihead_omega_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -4.6998, -15.8777, -20.5870,  -9.2172,  -9.4817,  15.4494],\n",
       "         [ -4.6211,  -0.8574, -24.4662,  -6.9085,  -4.5868,  23.2441],\n",
       "         [ -6.3106,   8.2177,  12.2194,   8.0759,   2.1534,  -5.5698]],\n",
       "\n",
       "        [[ 10.0815,   3.7914, -20.2564, -14.0807,  -4.5015,  15.9630],\n",
       "         [  4.6162,   8.4063, -23.4371, -14.6130, -13.7417,  13.2370],\n",
       "         [  0.9246, -14.5083,  13.1240,  -3.5708,   4.6327, -11.3191]],\n",
       "\n",
       "        [[  9.5033,  11.6041, -15.0547, -13.7880, -12.2019,  15.5950],\n",
       "         [  5.5354,   7.9537, -19.8211, -16.5260,  -4.5941,  11.4768],\n",
       "         [  0.4722,  14.0731,   3.7612,   1.9591,   1.2152, -21.3313]]],\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_omega_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0253, 0.0030, 0.1937, 0.5669, 0.2305, 0.3184],\n",
      "         [0.0643, 0.0732, 0.2077, 0.7419, 0.4645, 0.8195],\n",
      "         [0.1067, 0.2318, 0.4200, 0.7248, 0.2870, 0.7411]],\n",
      "\n",
      "        [[0.5161, 0.1682, 0.2072, 0.2101, 0.6372, 0.3536],\n",
      "         [0.4241, 0.4848, 0.2562, 0.1539, 0.0717, 0.1063],\n",
      "         [0.4673, 0.0022, 0.5052, 0.0673, 0.4760, 0.2292]],\n",
      "\n",
      "        [[0.4586, 0.8288, 0.5991, 0.2230, 0.1323, 0.3280],\n",
      "         [0.5116, 0.4420, 0.5361, 0.1042, 0.4638, 0.0742],\n",
      "         [0.4260, 0.7660, 0.0747, 0.2080, 0.2370, 0.0297]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "multihead_weights_2 = F.softmax(multihead_omega_2 / d_k**0.5, dim=0)\n",
    "print(multihead_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28]), torch.Size([6, 28]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 6]), torch.Size([3, 28, 6]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_weights_2.shape, multihead_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [3, 6] but got: [3, 28].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Datascience/Value-Driven-Data-Science/Value-Driven-Data-Science/00_Learning/03_Deep_Learning/Transformers-self-attention.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Datascience/Value-Driven-Data-Science/Value-Driven-Data-Science/00_Learning/03_Deep_Learning/Transformers-self-attention.ipynb#Y162sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m multihead_context_vector_2 \u001b[39m=\u001b[39m multihead_weights_2\u001b[39m.\u001b[39;49mmatmul(multihead_values)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Datascience/Value-Driven-Data-Science/Value-Driven-Data-Science/00_Learning/03_Deep_Learning/Transformers-self-attention.ipynb#Y162sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(multihead_context_vector_2\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/Datascience/Value-Driven-Data-Science/Value-Driven-Data-Science/00_Learning/03_Deep_Learning/Transformers-self-attention.ipynb#Y162sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(multihead_context_vector_2)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [3, 6] but got: [3, 28]."
     ]
    }
   ],
   "source": [
    "multihead_context_vector_2 = multihead_weights_2.matmul(multihead_values)\n",
    "\n",
    "print(multihead_context_vector_2.shape)\n",
    "print(multihead_context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary self-attention mechanism\n",
    "\n",
    "<img src=\"./img/summary_self_attention.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In self-attention, we work with the same input sequence. In cross-attention, we mix or combine two different input sequences. In the case of the original transformer architecture above, that’s the sequence returned by the encoder module on the left and the input sequence being processed by the decoder part on the right.\n",
    "\n",
    "Note that in cross-attention, the two input sequences $\\mathbf{x_{1}}$ and $\\mathbf{x_{2}}$ can have different numbers of elements. However, their embedding dimensions must match.\n",
    "\n",
    "The figure below illustrates the concept of cross-attention. If we set $\\mathbf{x_{1}}$ = $\\mathbf{x_{2}}$, this is equivalent to self-attention.\n",
    "\n",
    "<img src=\"./img/cross-attention.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>\n",
    "\n",
    "<img src=\"./img/cross-attention-summary.png\" alt=\"alt text\" \n",
    "  style=\"height: 80%;   \n",
    "    display: block;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    width: 80%;\"/>\n",
    "\n",
    "In code it will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_sentence.shape: torch.Size([6, 16])\n",
      "query.shape torch.Size([24])\n",
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "print(\"embedded_sentence.shape:\", embedded_sentence.shape)\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.rand(d_q, d)\n",
    "W_key = torch.rand(d_k, d)\n",
    "W_value = torch.rand(d_v, d)\n",
    "\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "print(\"query.shape\", query_2.shape)\n",
    "\n",
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only part that changes in cross attention is that we now have a second input sequence, for example, a second sentence with 8 instead of 6 input elements. Here, suppose this is a sentence with 8 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([8, 24])\n",
      "values.shape: torch.Size([8, 28])\n"
     ]
    }
   ],
   "source": [
    "embedded_sentence_2 = torch.rand(8, 16) # 2nd input sequence\n",
    "\n",
    "keys = W_key.matmul(embedded_sentence_2.T).T\n",
    "values = W_value.matmul(embedded_sentence_2.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that compared to self-attention, the keys and values now have 8 instead of 6 rows. Everything else stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
